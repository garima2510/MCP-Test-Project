{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948ef990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../mcp_project/mcp_chatbot_multi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../mcp_project/mcp_chatbot_multi.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.sessions: List[ClientSession] = [] # new\n",
    "        self.exit_stack = AsyncExitStack() # new\n",
    "        \n",
    "        # Initialize Azure OpenAI client\n",
    "        self.client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\"),\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "        \n",
    "        self.deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\")\n",
    "        self.available_tools: List[ToolDefinition] = [] # new\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {} # new\n",
    "\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            ) # new\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            ) # new\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "            \n",
    "            # List available tools for this session\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
    "            \n",
    "            for tool in tools: # new\n",
    "                self.tool_to_session[tool.name] = session\n",
    "                self.available_tools.append({\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self): # new\n",
    "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "        try:\n",
    "            with open(\"server_config.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            servers = data.get(\"mcpServers\", {})\n",
    "            \n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server configuration: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def convert_mcp_tools_to_openai_format(self, mcp_tools):\n",
    "        \"\"\"Convert MCP tool format to OpenAI function calling format\"\"\"\n",
    "        openai_functions = []\n",
    "        for tool in mcp_tools:\n",
    "            function_def = {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool[\"name\"],\n",
    "                    \"description\": tool[\"description\"],\n",
    "                    \"parameters\": tool[\"input_schema\"]\n",
    "                }\n",
    "            }\n",
    "            openai_functions.append(function_def)\n",
    "        return openai_functions\n",
    "    \n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role': 'user', 'content': query}]\n",
    "        \n",
    "        # Convert MCP tools to OpenAI format\n",
    "        openai_tools = self.convert_mcp_tools_to_openai_format(self.available_tools)\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.deployment_name,\n",
    "            messages=messages,\n",
    "            tools=openai_tools if openai_tools else None,\n",
    "            tool_choice=\"auto\" if openai_tools else None,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "        \n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            message = response.choices[0].message\n",
    "            \n",
    "            # Handle text response\n",
    "            if message.content:\n",
    "                print(message.content)\n",
    "                messages.append({'role': 'assistant', 'content': message.content})\n",
    "                \n",
    "            # Handle tool calls\n",
    "            if message.tool_calls:\n",
    "                messages.append({\n",
    "                    'role': 'assistant', \n",
    "                    'content': message.content,\n",
    "                    'tool_calls': message.tool_calls\n",
    "                })\n",
    "                \n",
    "                for tool_call in message.tool_calls:\n",
    "                    tool_name = tool_call.function.name\n",
    "                    tool_args = json.loads(tool_call.function.arguments)\n",
    "                    tool_id = tool_call.id\n",
    "                    \n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                    \n",
    "                    # Call a tool using the correct session\n",
    "                    session = self.tool_to_session[tool_name] # new\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "                    \n",
    "                    # Convert result to string if it's not already\n",
    "                    if hasattr(result, 'content'):\n",
    "                        result_content = str(result.content)\n",
    "                    else:\n",
    "                        result_content = str(result)\n",
    "                    \n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_id,\n",
    "                        \"content\": result_content\n",
    "                    })\n",
    "                \n",
    "                # Get next response from OpenAI\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.deployment_name,\n",
    "                    messages=messages,\n",
    "                    tools=openai_tools if openai_tools else None,\n",
    "                    tool_choice=\"auto\" if openai_tools else None,\n",
    "                    max_tokens=2048\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                # No more tool calls, we're done\n",
    "                process_query = False\n",
    "\n",
    "    \n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def cleanup(self): # new\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        # the mcp clients and sessions are not initialized using \"with\"\n",
    "        # like in the previous lesson\n",
    "        # so the cleanup should be manually handled\n",
    "        await chatbot.connect_to_servers() # new! \n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup() #new! \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ec204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
